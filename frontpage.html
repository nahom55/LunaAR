<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <title>Markerless AR with A-Frame and AR.js</title>
    <script src="https://aframe.io/releases/1.4.0/aframe.min.js"></script>
    <script src="https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/c-frame/aframe-extras@7.5.0/dist/aframe-extras.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" />
    <link rel="stylesheet" href="style-front.css">
  </head>
  <body style="margin: 0; overflow: hidden;">
    <!-- Hamburger Menu -->
    <div id="nav-menu">
      <a href=''>Home</a>
      <a href='new-wifi.html'>Setup wifi</a>
      <a href='devices.html'>Devices</a>
    </div>
    <!-- Initial Description Text -->
    <div id="initial-description"><div id="hamburger-menu">
      <i class="fas fa-bars"></i>
    </div>
    </div>
    <!-- A-Frame Scene with AR.js -->
    <a-scene
      embedded
      arjs="sourceType: image; sourceUrl: bbck.png; debugUIEnabled: false; trackingMethod: best;"
      renderer="logarithmicDepthBuffer: true; colorManagement: true, exposure: 1.0; toneMapping: ACESFilmic;"
      vr-mode-ui="enabled: false"
    >

      <!-- Add a 3D Human Model -->
      <a-entity
        id="human-model"
        gltf-model="url(test-anm.glb)"
        scale="0.019 0.019 0.019"
        position="0 -1.03 -1"
        animation-mixer="clip: *; timeScale: 0; loop: repeat" 
        visible="false" <!-- Initially hidden -->
      ></a-entity>

      <!-- AR Buttons -->
      <a-entity id="serviceButtons" visible="false">
        <!-- Buttons will be added here -->
      </a-entity>

      <!-- Audio Element -->
      <a-entity id="audioElement" sound="src: url(kid.mp3); autoplay: false;"></a-entity>

      <!-- Camera with cursor for interactions -->
      <a-entity camera look-controls>
        <a-cursor fuse="true" fuse-timeout="500" geometry="primitive: ring; radiusInner: 0; radiusOuter: 0"></a-cursor>
      </a-entity>
    </a-scene>

    <!-- Green background plane with buttons -->
    <footer>
      <div class="input-area">
        <button class="mic-button"><i class="fas fa-microphone"></i></button>
        <button class="video-button"><i class="fas fa-video"></i></button>
        <input type="text" placeholder="Type something" id="textInput">
        <button class="send-button"><i class="fas fa-paper-plane"></i></button>
      </div>
    </footer>

    <script>
      
      const micButton = document.querySelector('.mic-button');
      const textInput = document.getElementById('textInput');
      const responseArea = document.querySelector('.response-area');
      const RASA_API_URL = "https://34.229.142.15/webhooks/rest/webhook";
      let recognition;
      let transcript;
      let textValue;

      // Check if the browser supports the Web Speech API
      if ('webkitSpeechRecognition' in window) {
        const recognition = new webkitSpeechRecognition();
        recognition.continuous = false;
        recognition.interimResults = false;
        recognition.lang = 'en-US';

        recognition.onstart = function() {
          micButton.classList.add('active');
          micButton.style.backgroundColor = 'red';
          textInput.placeholder = "Listening...";
        };

        recognition.onresult = async function(event) {
          transcript = event.results[0][0].transcript;
          textInput.value = transcript;
          textValue = transcript;
          textInput.placeholder = "Type something";
          console.log(textValue);

          if (textValue == "Wi-Fi." || textValue == "Devices." || textValue == "Front.") {
            localStorage.setItem("page", textValue);
            window.parent.postMessage({ key: "page", value: textValue }, "*");
            console.log("from local storage: ", localStorage.getItem("page"));
            
          }

          // else if (textValue == "devices.") {
          //   localStorage.setItem("devices", textValue);
          //   window.parent.postMessage({ key: "devices", value: "devices" }, "*");
          //   console.log("from local storage: ", localStorage.getItem("devices"));
            
          // }

          // else if (textValue == "front.") {
          //   localStorage.setItem("front", textValue);
          //   window.parent.postMessage({ key: "frontpage", value: "frontpage" }, "*");
          //   console.log("from local storage: ", localStorage.getItem("frontpage"));
            
          // }

          // sendMessageToRasa(transcript);
        };

        recognition.onerror = function(event) {
          console.error('Speech recognition error', event.error);
          textInput.placeholder = "Error occurred. Please try again.";
        };

        recognition.onend = function() {
          micButton.classList.remove('active');
          micButton.style.backgroundColor = '#4CAF50';
          textInput.placeholder = "Type something";
        };

        micButton.addEventListener('click', function() {
          recognition.start();
        });
      } else {
        micButton.disabled = true;
        textInput.placeholder = 'Voice input not supported';
      }


      async function sendMessageToRasa(message) {
      
 

  

// Navigate based on intent

// const response = await fetch("https://rasa.qemsha.com/webhooks/rest/webhook", {

// method: "POST",

// headers: { "Content-Type": "application/json" },

// body: JSON.stringify({ sender: "test_user", message: message })

// });

// let intent;
// const data = await response.json();

// const botMessage = data[0]?.text || "No reply";
// console.log(botMessage.includes("H"));

// if (botMessage.includes("Goodbye!")){
//   intent = "goodbye";
// }

// if (botMessage.includes("Hey!")){
//   intent = "greet";
// }

// if (botMessage.includes("device")){
//   intent = "request_device_recommendation";
// }

// if (botMessage.includes("better") || botMessage.includes("assembling") || botMessage.includes("ready")){
//   intent = "router_state_detected";
// }

// if (botMessage.includes("camera")){
//   intent = "start_router_setup";
// }


// intent = data[1]?.custom?.intent || null;
// console.log("Intent detected:", intent);
// console.log("bot detected:", data);
// if (intent === "request_device_recommendation" || intent === "start_router_setup") {
//   textToSpeech("Could you please let me know your budget?");
// } else if (botMessage) {
//   textToSpeech(botMessage);
// }




// if (textInput.value == "hello.") {
//   // window.location.href = "https://devices.qemsha.com";
//   // const page = this.getAttribute('data-page');
//                 let page = "new-wifi";
//                 showPage(page);
//                 history.pushState({page}, '', `#${page}`);
// } 

// if (intent == "request_device_recommendation") {
//   window.location.href = "https://devices.qemsha.com";
// } 

  }

















      
    
      // function handleIntent(userMessage, botResponse) {
      //   if (userMessage.toLowerCase().includes("wi-fi")) {
      //     textToSpeech("Awesome! Let’s get you connected. First, let’s identify your device model could you point your phone’s camera at the router for me?");
      //     setupWiFi();
      //   } else if (userMessage.toLowerCase().includes("internet")) {
      //     textToSpeech("Let's Check your APN setting together");
      //     troubleshootInternet();
      //   } else if (userMessage.toLowerCase().includes("device") || userMessage.toLowerCase().includes("devices")) {
      //     textToSpeech("Let me show you availible Device");
      //     devices();
      //   } else if (userMessage.toLowerCase().includes("bye")) {
      //     sayGoodbye();
      //   } else {
      //     textToSpeech("I'm sorry, I can't help you with this request. Do you have another request?");
      //   }
      // }
    
      function setupWiFi() {
        window.location.href = "https://wifi.qemsha.com";
      }
    
      function troubleshootInternet() {
        window.location.href = "https://apn.qemsha.com";
      }
      
      function devices() {
        window.location.href = "https://devices.qemsha.com";
      }
    
      function sayGoodbye() {
        console.log("Goodbye! Have a great day!");
      }

      const hamburgerMenu = document.getElementById("hamburger-menu");
      const navMenu = document.getElementById("nav-menu");

      navMenu.addEventListener("click", function(e) {
        clicker(e);
      });
      
      function clicker(e) {
        e.preventDefault();
        const btn = e.target.textContent;
        if (btn == "Home") {
          window.location.href = "https://home.qemsha.com";
        }
        if (btn == "Devices") {
          window.location.href = "https://devices.qemsha.com";
        }
        if (btn == "Setup wifi") {
          window.location.href = "https://wifi.qemsha.com";
        }
      }
      
      hamburgerMenu.addEventListener("click", () => {
        navMenu.style.display = navMenu.style.display === "block" ? "none" : "block";
      });
      
      function playAudio() {
        const audioElement = document.getElementById("audioElement");
        audioElement.components.sound.playSound();
      }
 
      // Function to load lip-sync data from JSON file
      async function loadLipSyncData() {
        try {
          const response = await fetch('output.json');
          if (!response.ok) throw new Error('Failed to load lip-sync data');
          return await response.json();
        } catch (error) {
          console.error('Error loading lip-sync data:', error);
          return null;
        }
      }

      // Map Rhubarb visemes to Oculus Viseme IDs
      const rhubarbToOculusViseme = {
        A: 10, // aa
        B: 0,  // sil
        C: 7,  // SS
        D: 4,  // DD
        E: 11, // E
        F: 5,  // FF
        G: 6,  // CH
        H: 10, // aa
        X: 0,  // sil
      };

      // Map Oculus Visemes to blend shapes
      const visemeToBlendShape = {
        0: 'viseme_sil', // sil
        1: 'viseme_PP',  // PP
        2: 'viseme_FF',  // FF
        3: 'viseme_TH',  // TH
        4: 'viseme_DD',  // DD
        5: 'viseme_kk',  // kk
        6: 'viseme_CH',  // CH
        7: 'viseme_SS',  // SS
        8: 'viseme_nn',  // nn
        9: 'viseme_RR',  // RR
        10: 'viseme_aa', // aa
        11: 'viseme_E',  // E
        12: 'viseme_ih', // ih
        13: 'viseme_oh', // oh
        14: 'viseme_ou', // ou
      };

      // Function to animate the character's mouth
      function animateCharacterMouth(visemeId) {
        const visemeName = visemeToBlendShape[visemeId];
        console.log(`Animating viseme: ${visemeName}`);

        const humanModel = document.getElementById("human-model");
        humanModel.object3D.traverse((child) => {
          if (child.isMesh && child.morphTargetInfluences) {
            const morphTargetDictionary = child.morphTargetDictionary;

            // Reset all blend shapes to 0
            for (let i = 0; i < child.morphTargetInfluences.length; i++) {
              child.morphTargetInfluences[i] = 0;
            }

            // Apply the current viseme
            if (morphTargetDictionary && visemeName && morphTargetDictionary[visemeName] !== undefined) {
              child.morphTargetInfluences[morphTargetDictionary[visemeName]] = 1;
            }
          }
        });
      }

      // Modified function to start lip-sync animation
      async function startLipSyncAnimation() {
        const lipSyncData = await loadLipSyncData();
        if (!lipSyncData) {
          console.error('Could not load lip-sync data');
          return;
        }

        const audioElement = document.getElementById("audioElement");

        lipSyncData.mouthCues.forEach((cue) => {
          setTimeout(() => {
            const visemeId = rhubarbToOculusViseme[cue.value];
            animateCharacterMouth(visemeId);
          }, cue.start * 1000);
        });

        audioElement.components.sound.playSound();
      }

      // Show the 3D model, start animation, and play audio after 4 seconds
      setTimeout(async () => {
        const humanModel = document.getElementById("human-model");
        humanModel.setAttribute("visible", true);
        humanModel.setAttribute("animation-mixer", "timeScale: 1");
        await startLipSyncAnimation();
      }, 2000);

      // Better version that waits for voices to load
      function textToSpeech(message) {
  if (!window.speechSynthesis) {
    console.error("Speech synthesis not supported in this browser.");
    return null;
  }

  function speak() {
    const voices = window.speechSynthesis.getVoices();

    // Try to find a known female English voice
    const femaleVoice = voices.find(voice =>
      voice.name.toLowerCase().includes("zira") || // MS Zira (Windows)
      voice.name.toLowerCase().includes("susan") || // Google Susan
      voice.name.toLowerCase().includes("female") ||
      voice.name.toLowerCase().includes("woman") ||
      (voice.lang === "en-US" && voice.name.toLowerCase().includes("english")) ||
      (voice.lang === "en-US" && voice.name.toLowerCase().includes("us"))
    ) || voices.find(voice => voice.lang === "en-US") || voices[0]; // Fallbacks

    const utterance = new SpeechSynthesisUtterance(message);
    utterance.voice = femaleVoice;
    window.speechSynthesis.speak(utterance);
    return utterance;
  }

  // If voices aren't loaded yet, wait for them to load first
  if (window.speechSynthesis.getVoices().length === 0) {
    window.speechSynthesis.onvoiceschanged = speak;
  } else {
    return speak();
  }
}

 
      // Eye blinking functionality
      function animateEyes() {
        const humanModel = document.getElementById("human-model");
        humanModel.object3D.traverse((child) => {
          if (child.isMesh && child.morphTargetInfluences) {
            const morphTargetDictionary = child.morphTargetDictionary;

            if (morphTargetDictionary && morphTargetDictionary['eyeBlinkLeft'] !== undefined) {
              child.morphTargetInfluences[morphTargetDictionary['eyeBlinkLeft']] = 1;
              child.morphTargetInfluences[morphTargetDictionary['eyeBlinkRight']] = 1;
            }
          }
        });

        setTimeout(() => {
          humanModel.object3D.traverse((child) => {
            if (child.isMesh && child.morphTargetInfluences) {
              const morphTargetDictionary = child.morphTargetDictionary;

              if (morphTargetDictionary && morphTargetDictionary['eyeBlinkLeft'] !== undefined) {
                child.morphTargetInfluences[morphTargetDictionary['eyeBlinkLeft']] = 0;
                child.morphTargetInfluences[morphTargetDictionary['eyeBlinkRight']] = 0;
              }
            }
          });
        }, 200);
      }

      // Start periodic blinking
      setInterval(() => {
        animateEyes();
      }, 4000);
    </script>
  </body>
</html>
